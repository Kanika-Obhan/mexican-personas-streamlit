"""r_optimized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15bLkrgQW-nSRvDS5x_Xl04kduyMlfT8O
"""

# !pip install llama-index-llms-google-genai 

# -*- coding: utf-8 -*-
import streamlit as st
import re
import time
import json
import pandas as pd
from llama_index.llms.google_genai import GoogleGenAI
from google import genai
import os
import glob

# Folder configuration
PERSONAS_FOLDER = "Personas"
QUESTIONS_FOLDER = "Questions"
USER_INFO_FILE = "user_info.txt"

def load_user_info():
    """Extract username and gender from user_info.txt"""
    username = "User"
    user_gender = "unknown"
    try:
        with open(USER_INFO_FILE, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        for line in lines:
            line = line.strip()
            if line.lower().startswith('name:'):
                username = line.split(':', 1)[1].strip()
            elif line.lower().startswith('gender:'):
                user_gender = line.split(':', 1)[1].strip().lower()
    except Exception:
        pass
    return username, user_gender

def call_gemini_local(query, previous_conversation, gender, username, botname, bot_prompt, llm_api_key_string):
    try:
        full_prompt = (
            f"{bot_prompt}\n"
            f"Previous conversation: {previous_conversation[-1000:]}\n"
            f"{username}: {query}\n"
            f"{botname}:"
        )
        client = genai.Client(api_key=llm_api_key_string)
        response_text = ""
        for chunk in client.models.generate_content_stream(
            model="gemini-2.0-flash",
            contents=[full_prompt]
        ):
            if chunk.text:
                response_text += chunk.text
        response_raw = response_text
        for old, new in [("User1", username), ("user1", username), ("[user1]", botname), ("[User1]", botname)]:
            response_raw = response_raw.replace(old, new)
        return response_raw.strip()
    except json.JSONDecodeError:
        return f"JSON Decode Error: Unable to parse API response."
    except KeyError as e:
        return f"KeyError: {str(e)}. API response structure unexpected."

# ... (Keep get_persona_files, extract_relationship_from_filename, extract_bot_details_from_content, 
# load_persona_content, load_questions functions unchanged from previous versions) ...

# Initialize session state
if "user_info_loaded" not in st.session_state:
    st.session_state.username, st.session_state.user_gender = load_user_info()
    st.session_state.user_info_loaded = True

required_states = [
    'response_matrix', 'selected_persona', 'botname', 'bot_origin',
    'relationship', 'questions', 'csv_filename', 'bulk_running',
    'paused', 'current_question_index', 'previous_conversation',
    'user_questions', 'show_resume'
]

for state in required_states:
    if state not in st.session_state:
        st.session_state[state] = None if state == 'selected_persona' else False if 'running' in state else []

# UI Elements
persona_files = get_persona_files()

if persona_files:
    selected_file = st.selectbox("Select a persona:", persona_files)
    if selected_file != st.session_state.selected_persona:
        st.session_state.selected_persona = selected_file
        persona_content = load_persona_content(selected_file)
        st.session_state.botname, st.session_state.bot_origin = extract_bot_details_from_content(persona_content)
        relationship = extract_relationship_from_filename(selected_file)
        st.session_state.relationship = relationship
        st.session_state.questions = load_questions(relationship.split()[-1])

if st.session_state.selected_persona and st.session_state.questions:
    st.title(f"{st.session_state.botname} ({st.session_state.bot_origin}) {st.session_state.relationship.title()} Q&A")
    
    # Bulk Generation Section
    col1, col2 = st.columns(2)
    with col1:
        if st.button("Start Bulk Generation", disabled=st.session_state.bulk_running):
            st.session_state.bulk_running = True
            st.session_state.paused = False
            st.session_state.current_question_index = 0
            st.session_state.previous_conversation = []

    # Single Question Section
    with col2:
        user_question = st.text_input("Ask a single question:")
        if st.button("Ask") and user_question:
            if st.session_state.bulk_running and not st.session_state.paused:
                st.session_state.paused = True
                st.session_state.show_resume = True
                
            response = call_gemini_local(
                user_question, 
                st.session_state.previous_conversation,
                st.session_state.user_gender,
                st.session_state.username,
                st.session_state.botname,
                st.session_state.bot_prompt,
                "AIzaSyAWMudIst86dEBwP63BqFcy4mdjr34c87o"
            )
            
            st.session_state.user_questions.append({
                "question": user_question,
                "answer": response,
                "time": time.time()
            })
            st.session_state.previous_conversation += f"\n{user_question}\n{response}"

    # Bulk Generation Logic
    if st.session_state.bulk_running and not st.session_state.paused:
        if st.session_state.current_question_index < len(st.session_state.questions):
            progress = st.progress(st.session_state.current_question_index / len(st.session_state.questions))
            question = st.session_state.questions[st.session_state.current_question_index]
            
            response = call_gemini_local(
                question, 
                st.session_state.previous_conversation,
                st.session_state.user_gender,
                st.session_state.username,
                st.session_state.botname,
                st.session_state.bot_prompt,
                "AIzaSyAWMudIst86dEBwP63BqFcy4mdjr34c87o"
            )
            
            st.session_state.response_matrix.append([
                question, len(question), 0, response, 
                0, time.time(), f"{st.session_state.relationship} ({st.session_state.bot_origin})"
            ])
            st.session_state.previous_conversation += f"\n{question}\n{response}"
            st.session_state.current_question_index += 1
            st.rerun()
        else:
            st.session_state.bulk_running = False
            df = pd.DataFrame(st.session_state.response_matrix, columns=[
                "Question", "Length of Q", "Q Difficulty level", 
                "Answer", "Answer Quality", "Time Taken", "Persona"
            ])
            csv_filename = f"{st.session_state.botname.replace(' ', '_')}_{st.session_state.relationship.replace(' ', '_')}_qna.csv"
            df.to_csv(csv_filename, index=False)
            st.session_state.csv_filename = csv_filename
            st.success("Bulk generation completed!")

    # Resume Logic
    if st.session_state.paused and st.session_state.show_resume:
        if st.button("Resume Bulk Generation"):
            st.session_state.paused = False
            st.session_state.show_resume = False
            st.rerun()

    # Download Section
    if st.session_state.csv_filename and os.path.exists(st.session_state.csv_filename):
        with open(st.session_state.csv_filename, "rb") as f:
            st.download_button(
                label="Download CSV",
                data=f,
                file_name=os.path.basename(st.session_state.csv_filename),
                mime="text/csv"
            )

    # Conversation History
    st.subheader("Conversation History")
    if st.session_state.user_questions:
        for q in st.session_state.user_questions:
            st.markdown(f"**You**: {q['question']}")  
            st.markdown(f"**{st.session_state.botname}**: {q['answer']}")
            st.divider()

elif not persona_files:
    st.error(f"No persona files found in {PERSONAS_FOLDER} directory!")
